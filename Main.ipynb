{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "E2GA9g7ihjrj",
        "0sVgp6zbggnX",
        "CC8ArkK6kIjX"
      ],
      "mount_file_id": "1lBHohKbpZEuG15C3OKy5lHdLeTSq8adW",
      "authorship_tag": "ABX9TyOGmk3GzKXmw4tAC7cr8yZv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EjbejaranosAI/Deep-Learning/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install necesary packages"
      ],
      "metadata": {
        "id": "E2GA9g7ihjrj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUYLzAUtgFvZ",
        "outputId": "589053fa-a2e6-4b40-f54e-fa30883abd58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 182 kB 56.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 53.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 168 kB 61.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 55.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 50.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 55.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 162 kB 50.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 158 kB 40.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 55.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 48.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 66.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 57.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 55.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 55.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 55.4 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install -q pandas\n",
        "!pip install -q tensorflow\n",
        "!pip install -q opencv-python\n",
        "!pip install -q scikit-learn\n",
        "!pip install -q wandb\n",
        "!wandb login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download data"
      ],
      "metadata": {
        "id": "0sVgp6zbggnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset 256x256 \n",
        "!curl -o /content/drive/MyDrive/Colab_Notebooks_upc/5_DL/Delivery_2/MAMe_data_256.zip https://storage.hpai.bsc.es/mame-dataset/MAMe_data_256.zip\n",
        "# Download labels \n",
        "!curl -o /content/drive/MyDrive/Colab_Notebooks_upc/5_DL/Delivery_2/MAMe_metadata.zip https://storage.hpai.bsc.es/mame-dataset/MAMe_metadata.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIuqWfpdggA-",
        "outputId": "87b9be72-4be3-45a6-8570-eb567b9cc1ca"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  689M  100  689M    0     0  20.0M      0  0:00:34  0:00:34 --:--:-- 22.3M\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  848k  100  848k    0     0   579k      0  0:00:01  0:00:01 --:--:--  579k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset High-Resolution\n",
        "#!curl -o /content/drive/MyDrive/Colab_Notebooks_upc/5_DL/Delivery_2/MAMe_data.zip https://storage.hpai.bsc.es/mame-dataset/MAMe_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eq9dzrKQjKNr",
        "outputId": "7a53ed7c-f826-4e11-b964-88ce32425bf9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0  205G    0  869M    0     0  20.6M      0  2:49:13  0:00:42  2:48:31 23.7M^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get GPU information"
      ],
      "metadata": {
        "id": "CC8ArkK6kIjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6adBJBBkH9j",
        "outputId": "c298dcd3-5459-4118-97a4-e892cbc0f797"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "9HNwRC1vj56Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# api_key = 190989b8377e79d615c691b45c63d5fcc4e5b29f \n",
        "```"
      ],
      "metadata": {
        "id": "eIk62j-TgUfb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from matplotlib.rcsetup import validate_sketch\n",
        "\n",
        "import tensorflow\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "whI4qpkbgRRR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks_upc/5_DL/Delivery_2/MAMe_data_256.zip .\n",
        "!cp /content/drive/MyDrive/Colab_Notebooks_upc/5_DL/Delivery_2/MAMe_metadata.zip .\n",
        "\n",
        "!unzip /content/MAMe_metadata.zip\n",
        "!rm MAMe_metadata.zip\n",
        "\n",
        "!unzip /content/MAMe_data_256.zip\n",
        "!rm MAMe_data_256.zip"
      ],
      "metadata": {
        "id": "uRg5L_fjpRpR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions"
      ],
      "metadata": {
        "id": "jTpj87_YnlFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader\n",
        "class Mame_dataloader(tf.keras.utils.Sequence):\n",
        "    def __init__(self, df, x_col, y_col=None, batch_size=32, num_classes=None, shuffle=True):\n",
        "        self.batch_size = batch_size\n",
        "        self.df = df\n",
        "        self.indices = self.df.index.tolist()\n",
        "        self.num_classes = num_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.x_col = x_col\n",
        "        self.y_col = y_col\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "        X, y = self.__get_data(index)\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "    def __get_data(self, index):\n",
        "        \n",
        "        X = []\n",
        "        y = []\n",
        "        \n",
        "        img_batch = self.x_col[index].tolist()\n",
        "        label_batch = self.y_col[index].tolist()\n",
        "        \n",
        "        for img_name, label in zip(img_batch, label_batch):\n",
        "            img = cv2.imread(img_name) / 255.0\n",
        "            #img = img.resize((224, 224))\n",
        "            label = int(label)\n",
        "            lb = np.zeros(self.num_classes)\n",
        "            lb[label] = 1\n",
        "    \n",
        "            X.append(img)\n",
        "            y.append(lb)\n",
        "        X = np.array(X)\n",
        "        \n",
        "        return X, np.array(y)\n",
        "\n",
        "\n",
        "def preprocessing_metadata(path_metadata, path_data, path_labels):\n",
        "  print(\"\")\n",
        "  print(\"------------------Starting with pre-processing--------------------\")\n",
        "  print(\"\")\n",
        "        \n",
        "  df = pd.read_csv(path_metadata)\n",
        "  df['path'] = path_data\n",
        "  df['path image'] = df['path'] + df['Image file']\n",
        "\n",
        "  label_dataframe = pd.read_csv(path_labels)\n",
        "  label_dataframe.head()\n",
        "\n",
        "  header = ['target', 'label']\n",
        "  label_dataframe.loc[-1] = label_dataframe.columns\n",
        "  label_dataframe.columns = header\n",
        "  #labels.sort_values('target',ascending=False)\n",
        "  label_dataframe = label_dataframe.reset_index(drop=True)\n",
        "  # Remove blank spaces\n",
        "  label_dataframe['label'] = label_dataframe['label'].str.strip()\n",
        "  label_dataframe.head()\n",
        "\n",
        "  #Replace Medium names by labels \n",
        "  labels = label_dataframe['label'].values\n",
        "  new_labels = label_dataframe['target'].values\n",
        "\n",
        "  labels_dict = dict(zip(labels, new_labels))\n",
        "  df['Medium'] = df['Medium'].str.strip()\n",
        "  df['Medium'] = df.loc[:,'Medium'].replace(labels_dict) \n",
        "  print(df.head())\n",
        "  print(\"\")\n",
        "  print(\"------------------Pre-processing finished--------------------\")\n",
        "  print(\"\")    \n",
        "  \n",
        "  \n",
        "  return df\n",
        "\n",
        "def samplesByClass(dataframe,num_to_see:int):\n",
        "  print(\"\")\n",
        "  print(\"-----------------Dataframe grouped by Medium class--------------------\")\n",
        "  print(\"\")\n",
        "  dataframe['count'] = 1\n",
        "  num_by_class = dataframe.groupby('Medium').sum().reset_index()\n",
        "  num = num_by_class.head(num_to_see)\n",
        "  print(num.head())\n",
        "  print(\"\")\n",
        "  return num\n",
        "\n",
        "\n",
        "# Get the number of samples by subset\n",
        "def subsetByClass(df):\n",
        "  print(\"\")\n",
        "  print(\"-----------------Dataframe grouped by Subset class--------------------\")\n",
        "  print(\"\")\n",
        "  subset_by_class = df.groupby('Subset').sum().reset_index()\n",
        "  print(subset_by_class.head())\n",
        "  print(\"\")\n",
        "  "
      ],
      "metadata": {
        "id": "Dd6s7n8mj5CS"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing data"
      ],
      "metadata": {
        "id": "mA-2jR_1s4Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_data = '/content/data_256'\n",
        "path_toy_data = '/content/MAMe_toy_dataset.csv'\n",
        "path_labels = '/content/MAMe_labels.csv'\n",
        "path_metadata = '/content/MAMe_dataset.csv'\n",
        "\n",
        "# Generate dataframe with data ready to create subset list\n",
        "df_metadata = preprocessing_metadata(path_metadata,path_data, path_labels)   \n",
        "# Get the number of samples by medium class\n",
        "samplesByClass(df_metadata, 29)\n",
        "\n",
        "# Get the number of samples by subset class\n",
        "subsetByClass(df_metadata)\n",
        "\n",
        "# Generate dataframe for each subset\n",
        "train, test, val = split_dataframe_subset(df_metadata)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCWBhvACo71n",
        "outputId": "9b1d3456-e46e-43a1-fe40-bf938185a13a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------Starting with pre-processing--------------------\n",
            "\n",
            "   Image file Medium                      Museum Museum-based instance ID  \\\n",
            "0  436018.jpg     17  Metropolitan Museum of Art                29.100.60   \n",
            "1   11779.jpg     17  Metropolitan Museum of Art                 1982.373   \n",
            "2   19022.jpg     17  Metropolitan Museum of Art                 2006.418   \n",
            "3  435883.jpg     17  Metropolitan Museum of Art                 61.101.3   \n",
            "4   10481.jpg     17  Metropolitan Museum of Art                    09.95   \n",
            "\n",
            "  Subset   Width  Height  Product size  Aspect ratio               path  \\\n",
            "0  train  3144.0  3840.0    12072960.0        0.8187  /content/data_256   \n",
            "1  train  1707.0  2136.0     3646152.0        0.7992  /content/data_256   \n",
            "2  train  2845.0  3811.0    10842295.0        0.7465  /content/data_256   \n",
            "3  train  3811.0  2916.0    11112876.0        1.3069  /content/data_256   \n",
            "4  train  3811.0  2099.0     7999289.0        1.8156  /content/data_256   \n",
            "\n",
            "                    path image  \n",
            "0  /content/data_256436018.jpg  \n",
            "1   /content/data_25611779.jpg  \n",
            "2   /content/data_25619022.jpg  \n",
            "3  /content/data_256435883.jpg  \n",
            "4   /content/data_25610481.jpg  \n",
            "\n",
            "------------------Pre-processing finished--------------------\n",
            "\n",
            "\n",
            "-----------------Dataframe grouped by Medium class--------------------\n",
            "\n",
            "  Medium      Width     Height  Product size  Aspect ratio  count\n",
            "0      1  4535983.0  4706227.0  1.794163e+10     1517.3361   1450\n",
            "1      2  4266773.0  4614728.0  1.738901e+10     1423.6207   1450\n",
            "2      3  1518946.0  1502402.0  3.164427e+09     1148.7903   1063\n",
            "3      4  4461459.0  5040068.0  1.715484e+10     1393.6038   1450\n",
            "4      5  4595271.0  4663465.0  1.568556e+10     1526.1633   1450\n",
            "\n",
            "\n",
            "-----------------Dataframe grouped by Subset class--------------------\n",
            "\n",
            "  Subset       Width      Height  Product size  Aspect ratio  count\n",
            "0   test  44892520.0  47273946.0  1.637804e+11    16086.5621  15657\n",
            "1  train  57327376.0  60420429.0  2.073942e+11    20908.3109  20300\n",
            "2    val   4085438.0   4370598.0  1.501077e+10     1465.8332   1450\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ctHOHow4s1Lm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}